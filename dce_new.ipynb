{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dce_new.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGFXf3ncDen0CSEOZJsW0D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/and-rgr/contradiction_and_entailment/blob/main/dce_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkuI4rb8MyLD",
        "outputId": "ff621cef-5711-44dd-950f-33601fcc6222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "sprRiigmNJ8-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.expand_frame_repr', False)"
      ],
      "metadata": {
        "id": "wrU3HuLyNRCu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the TPU\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "except ValueError:\n",
        "    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n",
        "\n",
        "print('Number of replicas:', strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYv-jikfNWyG",
        "outputId": "eff7bdc5-f50e-4713-8a96-7b0d4503d05f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of replicas: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "train=pd.read_csv('train.csv')\n",
        "test=pd.read_csv('test.csv')\n",
        "\n",
        "na_train = len(train) - len(train.dropna())\n",
        "na_test = len(test) - len(test.dropna())\n",
        "\n",
        "print(\"check for missing rows:\", \"\\n\\ttrain data:\", na_train, \", test data:\", na_test, \"\\n\")\n",
        "\n",
        "# reduce data - optional\n",
        "train = train.iloc[:100]\n",
        "test = test.iloc[:50]\n",
        "\n",
        "print(\"train data shape: \", train.shape)\n",
        "print(\"test data shape: \", test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPttc8GNNYFv",
        "outputId": "58fcb12a-136c-4951-ee56-275fafcf0226"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check for missing rows: \n",
            "\ttrain data: 0 , test data: 0 \n",
            "\n",
            "train data shape:  (100, 6)\n",
            "test data shape:  (50, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import TFAutoModel,AutoTokenizer\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "# tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# learning_rate = 1e-5\n",
        "# batch_size = 32\n",
        "# warmup = 600\n",
        "# max_seq_length = 128\n",
        "# num_train_epochs = 3.0\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "pD1vBGT3NZqO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# is .batch_encode_plus needed here?\n",
        "# tf_batch = tokenizer()\n",
        "\n",
        "max_len = 100\n",
        "\n",
        "tf_batch = tokenizer.batch_encode_plus(\n",
        "    train[['premise','hypothesis']].values.tolist(),\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=max_len,\n",
        "    return_attention_mask=True,\n",
        "    # return_tensors=\"tf\"\n",
        ")"
      ],
      "metadata": {
        "id": "Mn6muQJ0NdH2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tf1=tf.convert_to_tensor(tf_batch['input_ids'],dtype=tf.int32)\n",
        "train_tf2=tf.convert_to_tensor(tf_batch['attention_mask'],dtype=tf.int32)\n",
        "train_input={'input_word_ids':train_tf1,'input_mask':train_tf2}"
      ],
      "metadata": {
        "id": "JxlR67htW59c"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TOKENIZATION DONE ###"
      ],
      "metadata": {
        "id": "NTVhaQN-eaNK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # what does this do?\n",
        "# # why training = True?\n",
        "# tf_outputs = tf_model(tf_batch, training = True)"
      ],
      "metadata": {
        "id": "vGFkbrBoWEog"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf_predictions = tf.nn.softmax(tf_outputs[0], axis=-1)"
      ],
      "metadata": {
        "id": "rIYvxE1FWX-g"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you have labels, you can provide them to the model, it will return a tuple with the loss and the final activations.\n",
        "# tf_outputs = tf_model(tf_batch, labels = tf.constant([1, 0]))"
      ],
      "metadata": {
        "id": "H1hMdZ0mWh1b"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### DistilBERT ###"
      ],
      "metadata": {
        "id": "27H8-5pAfLAK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with strategy.scope():\n",
        "    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "\n",
        "    tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "    # tf_model = TFAutoModel.from_pretrained(model_name)\n",
        "\n",
        "    embedding = tf_model([input_word_ids, input_mask])[0]\n",
        "    output = tf.keras.layers.Dense(3, activation = 'softmax')(embedding)\n",
        "\n",
        "    model = tf.keras.Model(inputs = [input_word_ids,input_mask], outputs = output)\n",
        "\n",
        "    model.compile(optimizer = tf.keras.optimizers.Adam(lr = 1e-5),\n",
        "                  loss = 'sparse_categorical_crossentropy',\n",
        "                  metrics = ['accuracy'])\n",
        "    \n",
        "    model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyL3ulS2fNTC",
        "outputId": "fc7eefb1-f2d0-4f67-cdd3-1d4af1cad968"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_39']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_word_ids (InputLayer)    [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " input_mask (InputLayer)        [(None, 100)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_distil_bert_for_sequence_cl  TFSequenceClassifie  66955010   ['input_word_ids[0][0]',         \n",
            " assification_1 (TFDistilBertFo  rOutput(loss=None,               'input_mask[0][0]']             \n",
            " rSequenceClassification)       logits=(None, 2),                                                 \n",
            "                                 hidden_states=None                                               \n",
            "                                , attentions=None)                                                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            9           ['tf_distil_bert_for_sequence_cla\n",
            "                                                                 ssification_1[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 66,955,019\n",
            "Trainable params: 66,955,019\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_model([input_word_ids, input_mask])[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRDo9Se8csuL",
        "outputId": "f982b29f-3d21-4ad1-db9c-fb55c95be4ef"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KerasTensor: shape=(None, 2) dtype=float32 (created by layer 'tf_distil_bert_for_sequence_classification_1')>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# early_stop = tf.keras.callbacks.EarlyStopping(patience=2,restore_best_weights=True, verbose=1, monitor='val_accuracy')"
      ],
      "metadata": {
        "id": "-05_m6LMshTx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.fit(train_input, train.label.values, epochs = 3, verbose = 1, batch_size = None, validation_split = 0.2, callbacks=[early_stop])\n",
        "\n",
        "model.fit(train_input, train.label.values, epochs = 3, verbose = 1, batch_size = None, validation_split = 0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKgXGyymfRnh",
        "outputId": "195830e9-7fa4-4a89-eefa-0f8ee775238e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "3/3 [==============================] - 66s 18s/step - loss: 1.1552 - accuracy: 0.3000 - val_loss: 1.1055 - val_accuracy: 0.4000\n",
            "Epoch 2/3\n"
          ]
        }
      ]
    }
  ]
}