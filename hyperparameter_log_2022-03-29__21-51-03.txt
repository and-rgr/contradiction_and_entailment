model_name = joeddav/xlm-roberta-large-xnli
tokenizer_length = 150
learning_rate = 2e-05
batch_factor = 8
validation_split = 0.25
kernel_initializer = lecun_normal
epochs = 5
patience = 2
l1 regularization = 0.0075
l2 regularization = 0.0025
